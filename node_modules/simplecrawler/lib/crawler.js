/*
 * Simplecrawler
 * https://github.com/cgiffard/node-simplecrawler
 *
 * Copyright (c) 2011-2015, Christopher Giffard
 *
 */

// Queue Dependency
var FetchQueue      = require("./queue.js"),
    CookieJar       = require("./cookies.js"),
    MetaInfo        = require("../package.json");

var http            = require("http"),
    https           = require("https"),
    EventEmitter    = require("events").EventEmitter,
    uri             = require("urijs"),
    zlib            = require("zlib"),
    util            = require("util"),
    iconv           = require("iconv-lite");

var QUEUE_ITEM_INITIAL_DEPTH = 1;

/*
    Public: Constructor for the crawler.

    host                -   Initial hostname/domain to begin crawling from. By
                            default, the crawl will be locked to this hostname.
    initialPath         -   Initial path to begin crawling from.
    initialPort         -   Port to begin crawling from.
    interval            -   Request interval for the crawler. Defaults to 250ms.

    Examples

        var crawler = new Crawler("example.com","/",80,500);

        var crawler = new Crawler("example.com");

    Returns the crawler object which has now been constructed.

*/
var Crawler = function(host, initialPath, initialPort, interval) {
    var crawler = this;

    // Data integrity checks
    if (initialPort && isNaN(initialPort)) {
        throw new Error("Port must be a number!");
    }

    // SETTINGS TO STUFF WITH
    // (not here! Do it when you create a `new Crawler()`)

    // Domain to crawl
    crawler.host            = host || "";

    // Gotta start crawling *somewhere*
    crawler.initialPath     = initialPath || "/";
    crawler.initialPort     = initialPort || 80;
    crawler.initialProtocol = "http";

    // Internal 'tick' interval for spawning new requests
    // (as long as concurrency is under cap)
    // One request will be spooled per tick, up to the concurrency threshold.
    crawler.interval        = interval || 250;

    // Maximum request concurrency. Be sensible. Five ties in with node's
    // default maxSockets value.
    crawler.maxConcurrency  = 5;

    // Maximum time we'll wait for headers
    crawler.timeout         = 5 * 60 * 1000;

    // Maximum time we'll wait for async listeners.
    crawler.listenerTTL     = 10 * 1000;

    // User Agent
    crawler.userAgent =
            "Node/" + MetaInfo.name + " " + MetaInfo.version +
            " (" + MetaInfo.repository.url + ")";

    // Queue for requests - FetchQueue gives us stats and other sugar
    // (but it's basically just an array)
    crawler.queue           = new FetchQueue();

    // Decode HTTP responses based on their Content-Type header or any
    // inline charset definition
    crawler.decodeResponses = false;

    // Do we filter by domain?
    // Unless you want to be crawling the entire internet, I would
    // recommend leaving this on!
    crawler.filterByDomain  = true;

    // Do we scan subdomains?
    crawler.scanSubdomains  = false;

    // Treat WWW subdomain the same as the main domain (and don't count
    // it as a separate subdomain)
    crawler.ignoreWWWDomain = true;

    // Or go even further and strip WWW subdomain from domains altogether!
    crawler.stripWWWDomain  = false;

    // Internal cachestore
    crawler.cache           = null;

    // Use an HTTP Proxy?
    crawler.useProxy        = false;
    crawler.proxyHostname   = "127.0.0.1";
    crawler.proxyPort       = 8123;
    crawler.proxyUser       = null;
    crawler.proxyPass       = null;

    // Support for HTTP basic auth
    crawler.needsAuth       = false;
    crawler.authUser        = "";
    crawler.authPass        = "";

    // Support for retaining cookies for parse duration
    crawler.acceptCookies   = true;
    crawler.cookies         = new CookieJar();

    // Support for custom headers...
    crawler.customHeaders   = {};

    // Domain Whitelist
    // We allow domains to be whitelisted, so cross-domain requests can be made.
    crawler.domainWhitelist = [];

    // Supported Protocols
    crawler.allowedProtocols = [
        /^http(s)?$/i,                  // HTTP & HTTPS
        /^(rss|atom|feed)(\+xml)?$/i    // RSS / XML
    ];

    // Max file size to download/store
    crawler.maxResourceSize = 1024 * 1024 * 16; // 16mb

    // Supported MIME-types
    // Matching MIME-types will be scanned for links
    crawler.supportedMimeTypes = [
        /^text\//i,
        /^application\/(rss|html|xhtml)?[\+\/\-]?xml/i,
        /^application\/javascript/i,
        /^xml/i
    ];

    // Download linked, but unsupported files (binary - images, documents, etc)
    crawler.downloadUnsupported = true;

    // URL Encoding setting...
    crawler.urlEncoding = "unicode";

    // Strip Querystring Parameters from URL
    crawler.stripQuerystring = false;

    // Regular expressions for finding URL items in HTML and text
    crawler.discoverRegex = [
        /\s(?:href|src)\s?=\s?(["']).*?\1/ig,
        /\s(?:href|src)\s?=\s?[^"'\s][^\s>]+/ig,
        /\s?url\((["']).*?\1\)/ig,
        /\s?url\([^"'].*?\)/ig,

        // This could easily duplicate matches above, e.g. in the case of
        // href="http://example.com"
        /http(s)?\:\/\/[^?\s><\'\"]+/ig,

        // This might be a bit of a gamble... but get hard-coded
        // strings out of javacript: URLs. They're often popup-image
        // or preview windows, which would otherwise be unavailable to us.
        // Worst case scenario is we make some junky requests.
        /^javascript\:[a-z0-9\$\_\.]+\(['"][^'"\s]+/ig,

        // Find resources in <meta> redirects. We need to wrap these RegExp's in
        // functions because we only want to return the first capture group, not
        // the entire match. And we need two RegExp's because the necessary
        // attributes on the <meta> tag can appear in any order
        function(string) {
            var match = string.match(/<\s*meta[^>]*http-equiv=["']{0,1}refresh["']{0,1}[^>]*content=["']{0,1}[^"'>]*url=([^"'>]*)["']{0,1}[^>]*>/i);
            return Array.isArray(match) ? [match[1]] : undefined;
        },
        function(string) {
            var match = string.match(/<\s*meta[^>]*content=["']{0,1}[^"'>]*url=([^"'>]*)["']{0,1}[^>]*http-equiv=["']{0,1}refresh["']{0,1}[^>]*>/i);
            return Array.isArray(match) ? [match[1]] : undefined;
        }
    ];

    // Whether to parse inside HTML comments
    crawler.parseHTMLComments = true;

    // Whether to parse inside script tags
    crawler.parseScriptTags = true;

    // Max depth parameter
    crawler.maxDepth = 0;

    // Whether to allow 'resources' greater than the max depth to be downloaded
    crawler.fetchWhitelistedMimeTypesBelowMaxDepth = false;

    // Ignore invalid SSL certificates
    crawler.ignoreInvalidSSL = false;

    // The HTTP / HTTPS agent used to crawl
    crawler.httpAgent       = http.globalAgent;
    crawler.httpsAgent      = https.globalAgent;

    // STATE (AND OTHER) VARIABLES NOT TO STUFF WITH
    var hiddenProps = {
        _openRequests: 0,
        _fetchConditions: [],
        _openListeners: 0
    };

    // Run the EventEmitter constructor
    EventEmitter.call(crawler);

    // Apply all the hidden props
    Object.keys(hiddenProps).forEach(function(key) {
        Object.defineProperty(crawler, key, {
            writable: true,
            enumerable: false,
            value: hiddenProps[key]
        });
    });
};

util.inherits(Crawler, EventEmitter);

/*
    Public: Starts or resumes the crawl. If the queue is empty, it adds a new
    queue item from which to begin crawling based on the initial configuration
    of the crawler itself. The crawler waits for process.nextTick to begin, so
    handlers and other properties can be altered or addressed before the crawl
    commences.

    Examples

        crawler.start();

    Returns the crawler object, to enable chaining.

*/
Crawler.prototype.start = function() {
    var crawler = this;

    // only if we haven't already got stuff in our queue...
    crawler.queue.getLength(function(err, length) {
        if (err) {
            throw err;
        }

        if (!length) {

            // Initialise our queue by pushing the initial request data into it...
            crawler.queue.add(
                crawler.initialProtocol,
                crawler.host,
                crawler.initialPort,
                crawler.initialPath,
                QUEUE_ITEM_INITIAL_DEPTH,
                function(error) {
                    if (error) {
                        throw error;
                    }
                });
        }

        process.nextTick(function() {
            crawler.crawlIntervalID =
                setInterval(
                    function() {
                        crawler.crawl(crawler);
                    },
                    crawler.interval);


            // Now kick off the initial crawl
            crawler.crawl();
        });
        crawler.running = true;
        crawler.emit("crawlstart");
    });

    return crawler;
};

/*
    Public: Determines whether the protocol is supported, given a URL.

    URL - URL with a protocol, for testing.

    Examples

        crawler.protocolSupported("http://google.com/") // true, by default
        crawler.protocolSupported("wss://google.com/")  // false, by default

    Returns a boolean, true if the protocol is supported - false if not.

*/
Crawler.prototype.protocolSupported = function(URL) {
    var protocol,
        crawler = this;

    try {
        protocol = uri(URL).protocol();

        // Unspecified protocol. Assume http
        if (!protocol) {
            protocol = "http";
        }

    } catch (e) {
        // If URIjs died, we definitely /do not/ support the protocol.
        return false;
    }

    return crawler.allowedProtocols.reduce(function(prev, protocolCheck) {
        return prev || !!protocolCheck.exec(protocol);
    }, false);
};

/*
    Public: Determines whether the mimetype is supported, given a mimetype

    MIMEType    - String containing MIME type to test

    Examples

        crawler.mimeTypeSupported("text/html") // true, by default
        crawler.mimeTypeSupported("application/octet-stream") // false, by default

    Returns a boolean, true if the MIME type is supported — false if not.

*/
Crawler.prototype.mimeTypeSupported = function(MIMEType) {
    var crawler = this;

    return crawler.supportedMimeTypes.reduce(function(prev, mimeCheck) {
        return prev || !!mimeCheck.exec(MIMEType);
    }, false);
};

/*
    Public: Determines whether the queueItem can be fetched from its depth

    In fact, the queueItem needs to be fetched before calling this (because we
    need its MIME type). This will just determine if we need to send an event
    for this item & if we need to fetch linked resources.

    If the queue item is a CSS or JS file, it will always be fetched (we need
    all images in CSS files, even if max depth is already reached). If it's an
    HTML page, we will check if max depth is reached or not.

        queueItem   - Queue item object to check

    Returns a boolean, true if the queue item can be fetched - false if not.

*/
Crawler.prototype.depthAllowed = function(queueItem) {
    var crawler = this;

    // Items matching this pattern will always be fetched, even if max depth
    // is reached
    var mimeTypesWhitelist = [
        /^text\/(css|javascript|ecmascript)/i,
        /^application\/javascript/i,
        /^application\/x-font/i,
        /^application\/font/i,
        /^image\//i,
        /^font\//i
    ];

    return crawler.maxDepth === 0 ||
           queueItem.depth <= crawler.maxDepth ||
               crawler.fetchWhitelistedMimeTypesBelowMaxDepth &&
               mimeTypesWhitelist.reduce(function(prev, mimeCheck) {
                   return prev || !!mimeCheck.exec(queueItem.stateData.contentType);
               }, false);
};

/*
    Public: Extracts protocol, host, port and resource (path) given a URL string.

    URL - String containing URL to process

    Examples

        var URLInfo = crawler.processURL("http://www.google.com/fish");

    Returns an object containing keys and values for "protocol", "host", "port",
    and "path".

*/
Crawler.prototype.processURL = function(URL, context) {
    var newURL,
        crawler = this;

    if (!context || typeof context !== "object") {
        context = {
            url: crawler.initialProtocol + "://" +
                crawler.host + ":" +
                crawler.initialPort + "/",
            depth: QUEUE_ITEM_INITIAL_DEPTH
        };
    }

    // If the URL didn't contain anything, don't fetch it.
    if (!(URL && URL.replace(/\s+/ig, "").length)) {
        return false;
    }

    // Check if querystring should be ignored
    if (crawler.stripQuerystring === true) {
        URL = crawler.removeQuerystring(URL);
    }

    if (crawler.stripWWWDomain && URL.match(/https?\:\/\/(www\.).*/i)) {
        URL = URL.replace("www.", "");
    }

    try {
        newURL =
            uri(URL)
                .absoluteTo(context.url)
                .normalize();

        if (crawler.urlEncoding === "iso8859") {
            newURL = newURL.iso8859();
        }

    } catch (e) {
        // Couldn't process the URL, since URIjs choked on it.
        return false;
    }

    // simplecrawler uses slightly different terminology to URIjs. Sorry!
    return {
        protocol: newURL.protocol() || "http",
        host:     newURL.hostname(),
        port:     newURL.port() || 80,
        path:     newURL.resource(),
        uriPath:  newURL.path(),
        depth:    context.depth + 1
    };
};

/*
    Private: Perform string replace operations on a URL string. Eg. removes
    HTML attribute fluff around actual URL, replaces leading "//" with
    absolute protocol etc.

    queueItem - Queue item corresponding to where the resource was found
    URL       - String to be cleaned up

    Examples

        cleanURL({protocol: "http"}, "url('//example.com/about') ")

    Returns a string.
 */
function cleanURL (queueItem, URL) {
    return URL
        .replace(/^(?:\s*href|\s*src)\s*=+\s*/i, "")
        .replace(/^\s*/, "")
        .replace(/^(['"])(.*)\1$/, "$2")
        .replace(/^url\((.*)\)/i, "$1")
        .replace(/^javascript\:\s*([a-z0-9]*\(["'](.*)["']\))*.*/i, "$2")
        .replace(/^(['"])(.*)\1$/, "$2")
        .replace(/^\((.*)\)$/, "$1")
        .replace(/^\/\//, queueItem.protocol + "://")
        .replace(/\&amp;/gi, "&")
        .replace(/\&#38;/gi, "&")
        .replace(/\&#x00026;/gi, "&")
        .split("#")
        .shift()
        .trim();
}

/*
    Public: Clean up a list of resources (normally provided by discoverResources).
    Also expands URL's that are relative to the current page.

    urlMatch  - Array of string resources
    queueItem - Queue item corresponding to where the resources were retrieved from

    Examples

        crawler.cleanExpandResources(["http://www.google.com", "/about", "mailto: example@example.com"])

    Returns an array of URL strings.
*/
Crawler.prototype.cleanExpandResources = function (urlMatch, queueItem) {
    var crawler = this;

    if (!urlMatch) {
        return [];
    }

    return urlMatch
        .map(cleanURL.bind(this, queueItem))
        .reduce(function(list, URL) {

            // Ensure URL is whole and complete
            try {
                URL = uri(URL)
                    .absoluteTo(queueItem.url || "")
                    .normalize()
                    .toString();
            } catch (e) {
                // But if URI.js couldn't parse it - nobody can!
                return list;
            }

            // If we hit an empty item, don't return it
            if (!URL.length) {
                return list;
            }

            // If we don't support the protocol in question
            if (!crawler.protocolSupported(URL)) {
                return list;
            }

            // Does the item already exist in the list?
            if (list.reduce(function(prev, current) {
                return prev || current === URL;
            }, false)) {
                return list;
            }

            return list.concat(URL);
        }, []);
};

/*
    Public: Discovers linked resources in an HTML, XML or text document.

    resourceData    - String containing document with linked resources.
    queueItem       - Queue item corresponding to document being searched.

    Examples

        crawler.discoverResources("http://www.google.com")
        crawler.discoverResources("<a href='...'>test</a>")

    Returns an array of the (string) resource URLs found in the document. If none
    were found, the array will be empty.

*/
Crawler.prototype.discoverResources = function(resourceData, queueItem) {
    // Convert to UTF-8
    // TODO: account for text-encoding.
    var resourceText = resourceData.toString("utf8"),
        crawler = this;

    if (!queueItem) {
        queueItem = {};
    }

    if (!queueItem.protocol) {
        queueItem.protocol = "http";
    }

    if (!crawler.parseHTMLComments) {
        resourceText = resourceText.replace(/<!--([\s\S]+?)-->/g, "");
    }

    if (!crawler.parseScriptTags) {
        resourceText = resourceText.replace(/<script(.*?)>([\s\S]*?)<\/script>/gi, "");
    }

    // Rough scan for URLs
    return crawler.discoverRegex
        .reduce(function(list, regex) {
            var resources = typeof regex === "function" ?
                regex(resourceText) :
                resourceText.match(regex);

            return list.concat(
                crawler.cleanExpandResources(resources, queueItem));
        }, [])
        .reduce(function(list, check) {
            if (list.indexOf(check) < 0) {
                return list.concat([check]);
            }

            return list;
        }, []);
};

/*
    Public: Determines based on crawler state whether a domain is valid for
    crawling.

    host - String containing the hostname of the resource to be fetched.

    Examples

        crawler.domainValid("127.0.0.1");
        crawler.domainValid("google.com");
        crawler.domainValid("test.example.com");

    Returns an true if the domain is valid for crawling, false if not.

*/
Crawler.prototype.domainValid = function(host) {
    var crawler = this;

    // If we're ignoring the WWW domain, remove the WWW for comparisons...
    if (crawler.ignoreWWWDomain) {
        host = host.replace(/^www\./i, "");
    }

    function domainInWhitelist(host) {

        // If there's no whitelist, or the whitelist is of zero length,
        // just return false.
        if (!crawler.domainWhitelist || !crawler.domainWhitelist.length) {
            return false;
        }

        // Otherwise, scan through it.
        return !!crawler.domainWhitelist.reduce(function(prev, cur) {

            // If we already located the relevant domain in the whitelist...
            if (prev) {
                return prev;
            }

            // If the domain is just equal, return true.
            if (host === cur) {
                return true;
            }

            // If we're ignoring WWW subdomains, and both domains,
            // less www. are the same, return true.
            if (crawler.ignoreWWWDomain && host === cur.replace(/^www\./i, "")) {
                return true;
            }

            // Otherwise, sorry. No dice.
            return false;
        }, false);
    }

    // Checks if the first domain is a subdomain of the second
    function isSubdomainOf(subdomain, host) {

        // Comparisons must be case-insensitive
        subdomain   = subdomain.toLowerCase();
        host        = host.toLowerCase();

        // If we're ignoring www, remove it from both
        // (if www is the first domain component...)
        if (crawler.ignoreWWWDomain) {
            subdomain = subdomain.replace(/^www./ig, "");
            host = host.replace(/^www./ig, "");
        }

        // They should be the same flipped around!
        return subdomain.split("").reverse().join("").substr(0, host.length) ===
                host.split("").reverse().join("");
    }

           // If we're not filtering by domain, just return true.
    return !crawler.filterByDomain ||
           // Or if the domain is just the right one, return true.
           host === crawler.host ||
           // Or if we're ignoring WWW subdomains, and both domains,
           // less www. are the same, return true.
           crawler.ignoreWWWDomain &&
               crawler.host.replace(/^www\./i, "") ===
                   host.replace(/^www\./i, "") ||
           // Or if the domain in question exists in the domain whitelist,
           // return true.
           domainInWhitelist(host) ||
           // Or if we're scanning subdomains, and this domain is a subdomain
           // of the crawler's set domain, return true.
           crawler.scanSubdomains && isSubdomainOf(host, crawler.host);
};

/*
    Public: Given a text or HTML document, initiates discovery of linked
    resources in the text, and queues the resources if applicable. Emits
    "discoverycomplete". Not to be confused with `crawler.discoverResources`,
    despite the `discoverResources` function being the main component of this
    one, since this function queues the resources in addition to
    discovering them.

    resourceData    - Text document containing linked resource URLs.
    queueItem       - Queue item from which the resource document was derived.
    decompressed    - Content is already decompressed (default: false)

    Emits

        gziperr
        discoverycomplete

    Examples

        crawler.queueLinkedItems("<a href='...'>test</a>",queueItem);

    Returns the crawler object for chaining.

*/
Crawler.prototype.queueLinkedItems = function(resourceData, queueItem, decompressed) {
    var crawler = this,
        resources = [];

    if (!decompressed &&
        queueItem.stateData &&
        queueItem.stateData.headers["content-encoding"] && (
        queueItem.stateData.headers["content-encoding"].match(/gzip/) ||
        queueItem.stateData.headers["content-encoding"].match(/deflate/))) {

        return zlib.unzip(resourceData, function(err, newData) {
            if (err) {
                return crawler.emit("gziperror", queueItem, err, resourceData);
            }

            crawler.queueLinkedItems(newData, queueItem, true);
        });
    }

    resources = crawler.discoverResources(resourceData, queueItem);

    // Emit discovered resources. ie: might be useful in building a graph of
    // page relationships.
    crawler.emit("discoverycomplete", queueItem, resources);

    resources.forEach(function(url) {
        crawler.queueURL(url, queueItem);
    });

    return crawler;
};

/*
    Public: Given a single URL, this function cleans, validates, parses it and
    adds it to the queue. This is the best and simplest way to add an item to
    the queue.

    url         - URL to be queued.
    queueItem   - Queue item from which the resource was linked.

    Emits

        queueduplicate
        queueerror
        queueadd

    Examples

        crawler.queueURL("http://www.google.com/",queueItem);

    Returns a boolean value indicating whether the URL was successfully queued
    or not.

*/
Crawler.prototype.queueURL = function(url, queueItem) {
    var crawler = this,
        parsedURL = typeof url === "object" ? url : crawler.processURL(url, queueItem);

    // URL Parser decided this URL was junky. Next please!
    if (!parsedURL) {
        return false;
    }

    // Pass this URL past fetch conditions to ensure the user thinks it's valid
    var fetchDenied = false;
    fetchDenied = crawler._fetchConditions.reduce(function(prev, callback) {
        return prev || !callback(parsedURL, queueItem);
    }, false);

    if (fetchDenied) {
        // Fetch Conditions conspired to block URL
        return false;
    }

    // Check the domain is valid before adding it to the queue
    if (crawler.domainValid(parsedURL.host)) {
        crawler.queue.add(
            parsedURL.protocol,
            parsedURL.host,
            parsedURL.port,
            parsedURL.path,
            parsedURL.depth,
            function queueAddCallback(error, newQueueItem) {
                if (error) {
                    // We received an error condition when adding the callback
                    if (error.code && error.code === "DUP") {
                        return crawler.emit("queueduplicate", parsedURL);
                    }

                    return crawler.emit("queueerror", error, parsedURL);
                }

                crawler.emit("queueadd", newQueueItem, parsedURL);
                newQueueItem.referrer = queueItem ? queueItem.url : null;
            }
        );
    }

    return true;
};

/*
    Public: The guts of the crawler: takes a queue item and spools a request for
    it, downloads, caches, and fires events based on the result of the request.
    It kicks off resource discovery and queues any new resources found.

    queueItem   - Queue item to be fetched.

    Emits
        fetchstart
        fetchheaders
        fetchcomplete
        fetchdataerror
        notmodified
        fetchredirect
        fetch404
        fetcherror
        fetchclienterror

    Examples

        crawler.fetchQueueItem(queueItem);

    Returns the crawler object for chaining.

*/
Crawler.prototype.fetchQueueItem = function(queueItem) {
    var crawler = this;
    crawler._openRequests++;

    // Variable declarations
    var requestOptions,
        clientRequest,
        timeCommenced;

    // Mark as spooled
    queueItem.status = "spooled";
    var client = queueItem.protocol === "https" ? https : http;
    var agent  = queueItem.protocol === "https" ? crawler.httpsAgent : crawler.httpAgent;

    // Up the socket limit if required.
    if (agent.maxSockets < crawler.maxConcurrency) {
        agent.maxSockets = crawler.maxConcurrency;
    }

    // Extract request options from queue;
    var requestHost = queueItem.host,
        requestPort = queueItem.port,
        requestPath = queueItem.path;

    // Are we passing through an HTTP proxy?
    if (crawler.useProxy) {
        requestHost = crawler.proxyHostname;
        requestPort = crawler.proxyPort;
        requestPath = queueItem.url;
    }

    // Load in request options
    requestOptions = {
        method: "GET",
        host:   requestHost,
        port:   requestPort,
        path:   requestPath,
        agent:  agent,
        headers: {
            "User-Agent":   crawler.userAgent,
            "Host":         queueItem.host + (
                                queueItem.port !== 80 ?
                                    ":" + queueItem.port :
                                    ""
                            )
        }
    };

    if (queueItem.referrer) {
        requestOptions.headers.Referer = queueItem.referrer;
    }

    // If port is one of the HTTP/HTTPS defaults, delete the option to avoid conflicts
    if (requestOptions.port === 80 || requestOptions.port === 443) {
        delete requestOptions.port;
    }

    // Add cookie header from cookie jar if we're configured to
    // send/accept cookies
    if (crawler.acceptCookies && crawler.cookies.getAsHeader()) {
        requestOptions.headers.cookie =
            crawler.cookies.getAsHeader(queueItem.host, queueItem.path);
    }

    // Add auth headers if we need them
    if (crawler.needsAuth) {
        var auth = crawler.authUser + ":" + crawler.authPass;

        // Generate auth header
        auth = "Basic " + new Buffer(auth).toString("base64");
        requestOptions.headers.Authorization = auth;
    }

    // Add proxy auth if we need it
    if (crawler.proxyUser !== null && crawler.proxyPass !== null) {
        var proxyAuth = crawler.proxyUser + ":" + crawler.proxyPass;

        // Generate auth header
        proxyAuth = "Basic " + new Buffer(proxyAuth).toString("base64");
        requestOptions.headers["Proxy-Authorization"] = proxyAuth;
    }

    // And if we've got any custom headers available
    if (crawler.customHeaders) {
        for (var header in crawler.customHeaders) {
            if (!crawler.customHeaders.hasOwnProperty(header)) {
                continue;
            }

            requestOptions.headers[header] = crawler.customHeaders[header];
        }
    }

    // Apply the ignoreInvalidSSL setting to https connections
    if (client === https && crawler.ignoreInvalidSSL === true) {
        client.rejectUnauthorized = false;
        client.strictSSL = false;
    }

    // Emit fetchstart event - gives the user time to mangle the request options
    // if required.
    crawler.emit("fetchstart", queueItem, requestOptions);

    // Record what time we started this request
    timeCommenced = Date.now();

    // Get the resource!
    clientRequest =
        client.request(requestOptions, function(response) {
            crawler.handleResponse(queueItem, response, timeCommenced);
        });

    clientRequest.end();

    clientRequest.setTimeout(crawler.timeout, function() {
        if (queueItem.fetched) {
            return;
        }

        if (crawler.running && !queueItem.fetched) {
            crawler._openRequests--;
        }

        queueItem.fetched = true;
        queueItem.status = "timeout";
        crawler.emit("fetchtimeout", queueItem, crawler.timeout);
        clientRequest._crawlerHandled = true;
        clientRequest.abort();
    });

    clientRequest.on("error", function(errorData) {

        // This event will be thrown if we manually aborted the request,
        // but we don't want to do anything in that case.
        if (clientRequest._crawlerHandled) {
            return;
        }

        if (crawler.running && !queueItem.fetched) {
            crawler._openRequests--;
        }

        // Emit 5xx / 4xx event
        queueItem.fetched = true;
        queueItem.stateData.code = 599;
        queueItem.status = "failed";
        crawler.emit("fetchclienterror", queueItem, errorData);
    });

    return crawler;
};


/*
    Decode string buffer based on a complete Content-Type header. Will also look
    for an embedded <meta> tag with a charset definition, but the Content-Type
    header is prioritized, see:
    https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta#attr-charset

    Examples

        crawler.decodeBuffer(responseBuffer, "text/html; charset=tis-620");

    Returns the decoded buffer.

*/
Crawler.prototype.decodeBuffer = function (buffer, contentTypeHeader) {
    contentTypeHeader = contentTypeHeader || "";

    var embeddedEncoding = /<meta.*charset=["']{0,1}([^"'>]*)["']{0,1}\s*\/{0,1}>/i.exec(buffer.toString(undefined, 0, 512)) || [],
        encoding = contentTypeHeader.split("charset=")[1] || embeddedEncoding[1] || contentTypeHeader;

    encoding = iconv.encodingExists(encoding) ? encoding : "utf-8";

    return iconv.decode(buffer, encoding);
};


/*
    Public: Given a queueItem and a matching response object, the crawler will
    handle downloading the resource, queueing of linked items, etc.

    Examples

        // Passing in a response from `request`
        request(queueItem.url, function(err, res, body) {
            crawler.handleResponse(queueItem, res);
        });

    Returns the crawler object for chaining.

*/
Crawler.prototype.handleResponse = function(queueItem, response, timeCommenced) {
    var crawler = this,
        dataReceived = false,
        timeHeadersReceived,
        timeDataReceived,
        parsedURL,
        responseBuffer,
        responseLength,
        responseLengthReceived = 0,
        contentType,
        stateData = queueItem.stateData;

    // Record what time we first received the header information
    timeHeadersReceived = Date.now();

    // If we weren't passed a time of commencement, assume Now()
    timeCommenced = timeCommenced || Date.now();

    responseLength = parseInt(response.headers["content-length"], 10);
    responseLength = !isNaN(responseLength) ? responseLength : 0;

    // Save timing and content some header information into queue
    stateData.requestLatency    = timeHeadersReceived - timeCommenced;
    stateData.requestTime       = timeHeadersReceived - timeCommenced;
    stateData.contentLength     = responseLength;
    stateData.contentType       = contentType = response.headers["content-type"];
    stateData.code              = response.statusCode;
    stateData.headers           = response.headers;

    // Do we need to save cookies? Were we sent any?
    if (crawler.acceptCookies && response.headers.hasOwnProperty("set-cookie")) {
        try {
            crawler.cookies.addFromHeaders(response.headers["set-cookie"]);
        } catch (error) {
            crawler.emit("cookieerror", queueItem, error, response.headers["set-cookie"]);
        }
    }

    // Emit header receive event
    crawler.emit("fetchheaders", queueItem, response);

    // Ensure response length is reasonable...
    responseLength =
        responseLength > 0 ? responseLength : crawler.maxResourceSize;

    queueItem.stateData.contentLength = responseLength;

    // Function for dealing with 200 responses
    function processReceivedData() {
        if (dataReceived || queueItem.fetched) {
            return;
        }

        responseBuffer = responseBuffer.slice(0, responseLengthReceived);
        dataReceived = true;

        timeDataReceived = new Date().getTime();

        queueItem.fetched = true;
        queueItem.status = "downloaded";

        // Save state information
        stateData.downloadTime      = timeDataReceived - timeHeadersReceived;
        stateData.requestTime       = timeDataReceived - timeCommenced;
        stateData.actualDataSize    = responseBuffer.length;
        stateData.sentIncorrectSize = responseBuffer.length !== responseLength;

        // First, save item to cache (if we're using a cache!)
        if (crawler.cache !== null && crawler.cache.setCacheData instanceof Function) {
            crawler.cache.setCacheData(queueItem, responseBuffer);
        }

        // Is the item allowed by depth conditions ?
        if (crawler.depthAllowed(queueItem)) {
            var responseBody =
                crawler.decodeResponses ? crawler.decodeBuffer(responseBuffer, stateData.contentType) : responseBuffer;

            crawler.emit("fetchcomplete", queueItem, responseBody, response);

            // We only process the item if it's of a valid mimetype
            // and only if the crawler is set to discover its own resources
            if (crawler.mimeTypeSupported(contentType) && crawler.discoverResources) {
                crawler.queueLinkedItems(responseBuffer, queueItem);
            }
        }

        crawler._openRequests--;
    }

    function receiveData(chunk) {
        if (!chunk.length || dataReceived) {
            return;
        }

        if (responseLengthReceived + chunk.length > responseBuffer.length) {
            // Oh dear. We've been sent more data than we were initially told.
            // This could be a mis-calculation, or a streaming resource.
            // Let's increase the size of our buffer to match, as long as it isn't
            // larger than our maximum resource size.

            if (responseLengthReceived + chunk.length <= crawler.maxResourceSize) {

                // Create a temporary buffer with the new response length, copy
                // the old data into it and replace the old buffer with it
                var tmpNewBuffer = new Buffer(responseLengthReceived + chunk.length);
                responseBuffer.copy(tmpNewBuffer, 0, 0, responseBuffer.length);
                chunk.copy(tmpNewBuffer, responseBuffer.length, 0, chunk.length);
                responseBuffer = tmpNewBuffer;
            } else {

                // The response size exceeds maxResourceSize. Throw event and
                // ignore. We'll then deal with the data that we have.
                response.socket.destroy();
                crawler.emit("fetchdataerror", queueItem, response);
            }
        } else {
            chunk.copy(responseBuffer, responseLengthReceived, 0, chunk.length);
        }

        responseLengthReceived += chunk.length;
    }

    // We already know that the response will be too big
    if (responseLength > crawler.maxResourceSize) {
        queueItem.fetched = true;
        crawler._openRequests--;

        response.socket.destroy();
        crawler.emit("fetchdataerror", queueItem, response);

    // We should just go ahead and get the data
    } else if (response.statusCode >= 200 && response.statusCode < 300) {

        queueItem.status = "headers";

        // Create a buffer with our response length
        responseBuffer = new Buffer(responseLength);

        // Only if we're prepared to download non-text resources...
        if (crawler.downloadUnsupported ||
            crawler.mimeTypeSupported(contentType)) {

            response.on("data", receiveData);
            response.on("end", processReceivedData);
        } else {
            queueItem.fetched = true;
            crawler._openRequests--;

            response.socket.destroy();
        }

    // We've got a not-modified response back
    } else if (response.statusCode === 304) {

        if (crawler.cache !== null && crawler.cache.getCacheData) {
            // We've got access to a cache
            crawler.cache.getCacheData(queueItem, function(cacheObject) {
                crawler.emit("notmodified", queueItem, response, cacheObject);
            });
        } else {
            // Emit notmodified event. We don't have a cache available, so
            // we don't send any data.
            crawler.emit("notmodified", queueItem, response);
        }

    // If we should queue a redirect
    } else if (response.statusCode >= 300 && response.statusCode < 400 &&
                    response.headers.location) {

        queueItem.fetched = true;
        queueItem.status = "redirected";

        // Parse the redirect URL ready for adding to the queue...
        parsedURL = crawler.processURL(response.headers.location, queueItem);

        // Emit redirect event
        crawler.emit("fetchredirect", queueItem, parsedURL, response);

        // Clean URL, add to queue...
        crawler.queueURL(parsedURL, queueItem);
        response.socket.end();

        crawler._openRequests--;

    // Ignore this request, but record that we had a 404
    } else if (response.statusCode === 404 || response.statusCode === 410) {
        queueItem.fetched = true;
        queueItem.status = "notfound";

        // Emit 404 event
        crawler.emit("fetch404", queueItem, response);
        response.socket.end();

        crawler._openRequests--;

    // And oh dear. Handle this one as well. (other 400s, 500s, etc)
    } else {
        queueItem.fetched = true;
        queueItem.status = "failed";

        // Emit 5xx / 4xx event
        crawler.emit("fetcherror", queueItem, response);
        response.socket.destroy();

        crawler._openRequests--;
    }

    return crawler;
};

/*
    Public: The main crawler runloop. Fires at the interval specified in the
    crawler configuration, when the crawl is running. May be manually fired.
    This function initiates fetching of a queue item if there are enough workers
    to do so and there are unfetched items in the queue.

    Examples

        crawler.crawl();

    Returns the crawler object for chaining.

*/
Crawler.prototype.crawl = function() {
    var crawler = this;

    if (crawler._openRequests > crawler.maxConcurrency) {
        return [];
    }

    crawler.queue.oldestUnfetchedItem(function(err, queueItem) {    // eslint-disable-line

        if (queueItem) {
            crawler.fetchQueueItem(queueItem);

        } else if (!crawler._openRequests && !crawler._openListeners) {

            crawler.queue.complete(function(err, completeCount) {
                if (err) {
                    throw err;
                }

                crawler.queue.getLength(function(err, length) {
                    if (err) {
                        throw err;
                    }

                    if (completeCount === length) {
                        crawler.emit("complete");
                        crawler.stop();
                    }
                });
            });
        }
    });

    return crawler;
};

/*
    Public: Stops the crawler, terminating the crawl runloop.

    Examples

        crawler.stop();

    Returns the crawler object for chaining.

*/
Crawler.prototype.stop = function() {
    var crawler = this;
    clearInterval(crawler.crawlIntervalID);
    crawler.running = false;
    return crawler;
};

/*
    Public: Holds the crawler in a 'running' state, preventing the `complete`
    event from firing until the callback this function returns has been executed,
    or a predetermined timeout (as specified by `crawler.listenerTTL`) has
    elapsed.

    Examples

        crawler.on("fetchcomplete",function(queueItem,data) {
            continue = this.wait();
            doSomethingThatTakesAlongTime(function callback() {
                continue();
            });
        });

    Returns callback which will allow the crawler to continue.

*/
Crawler.prototype.wait = function() {
    var crawler = this,
        cleared = false,
        timeout =
            setTimeout(function() {
                if (cleared) {
                    return;
                }
                cleared = true;
                crawler._openListeners--;
            }, crawler.listenerTTL);

    crawler._openListeners++;

    return function() {
        if (cleared) {
            return;
        }
        cleared = true;
        crawler._openListeners--;
        clearTimeout(timeout);
    };
};

/*
    Public: Given a function, this method adds it to an internal list maintained
    by the crawler to be executed against each URL to determine whether it should
    be fetched or not.

    callback -  Function to be called when evaluating a URL. This function is
                passed an object containing the protocol, hostname, port, and path
                of a resource to be fetched. It can determine whether it should
                be requested or not by returning a boolean - false for no, true
                for yes.

    Examples

        crawler.addFetchCondition(function(parsedURL) {
            return (parsedURL.host !== "evildomain.com");
        });

    Returns the ID of the fetch condition - used for removing it from the crawler
    later.

*/
Crawler.prototype.addFetchCondition = function(callback) {
    var crawler = this;
    if (callback instanceof Function) {
        crawler._fetchConditions.push(callback);
        return crawler._fetchConditions.length - 1;
    }
    throw new Error("Fetch Condition must be a function.");
};

/*
    Public: Given the ID of an existing fetch condition, this function removes
    it from the crawler's internal list of conditions.

    index - ID of fetch condition to be removed.

    Examples

        crawler.removeFetchCondition(3);

    Returns true if the fetch condition was removed, and throws an error if it
    could not be found.

*/
Crawler.prototype.removeFetchCondition = function(index) {
    var crawler = this;
    if (crawler._fetchConditions[index] &&
        crawler._fetchConditions[index] instanceof Function) {

        return !!crawler._fetchConditions.splice(index, 1);
    }
    throw new Error("Unable to find indexed Fetch Condition.");
};

/*
    Public: Given a URL it will remove the querstring if it exists.

    url - URL from which to remove the querystring

    Examples

        crawler.removeQuerystring(url);

    Returns URL without querystring if it exists

*/
Crawler.prototype.removeQuerystring = function(url) {
    if (url.indexOf("?") > -1) {
        return url.substr(0, url.indexOf("?"));
    }
    return url;
};

module.exports = Crawler;
